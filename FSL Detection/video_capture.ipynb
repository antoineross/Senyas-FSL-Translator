{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3169708a-daf6-42b4-a046-5fae612b59b3",
   "metadata": {},
   "source": [
    "1. Installing Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e6b4af-3ad5-40af-831d-0705f45599cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow==2.6 opencv-python mediapipe==0.9.1.0 scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b4d1f87-6208-4b65-ba8e-ad184100d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import mediapipe as mp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a64a0e8f-b515-4056-af2d-9d97d68baf3a",
   "metadata": {},
   "source": [
    "2. Capturing video and Drawing keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd639cf-e4c7-4219-a928-040a4640a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keypoints Portion\n",
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
    "    \n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8c6027a",
   "metadata": {},
   "source": [
    "3. Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "551ab2ff-8a66-485e-8134-3fe1d8154236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Keypoints into a numpy array\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4) \n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh]) \n",
    "# why is it important to flatten the array?\n",
    "# it is because we want to have a single row of data for each image\n",
    "\n",
    "# why is it important to concatenate? \n",
    "# it is because we want to have a single row of data for each image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7287952",
   "metadata": {},
   "source": [
    "4. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfbdb67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data_Maura') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['ako',  'ikaw', 'maganda', 'magandang umaga', 'hi'])\n",
    "# actions = np.array(['ako',  'ikaw', 'maganda', 'magandang umaga', 'hi', 'hindi', 'oo', 'salamat', 'bakit', 'kamusta'])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 4\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca0cff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions: \n",
    "    for sequence in range(no_sequences):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e96e6048",
   "metadata": {},
   "source": [
    "5. Collect Videos and Store them under videos/signed_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0270213",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array(['ako',  'ikaw', 'maganda', 'magandang umaga', 'hi', 'hindi', 'oo', 'salamat', 'bakit', 'kamusta'])\n",
    "for action in actions: \n",
    "    for sequence in range(no_sequences):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(\"videos\", \"signed_videos\", action))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fabb3e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting video collection\n",
      "Collection of ako is starting in 1 second.\n",
      "Collecting data for ako sequence: 0\n",
      "Collecting data for ako sequence: 1\n",
      "Collecting data for ako sequence: 2\n",
      "Collecting data for ako sequence: 3\n",
      "Collection of ikaw is starting in 1 second.\n",
      "Collecting data for ikaw sequence: 0\n",
      "Collecting data for ikaw sequence: 1\n",
      "Collecting data for ikaw sequence: 2\n",
      "Collecting data for ikaw sequence: 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3984\\2950868258.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'frame'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"Starting video collection\")\n",
    "\n",
    "for action in actions:\n",
    "    print(\"Collection of\", action,  \"is starting in 1 second.\")\n",
    "    cv2.waitKey(500)\n",
    "    for sequence in range(no_sequences):\n",
    "        frames = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            if frame_num==0:\n",
    "                print(\"Collecting data for\", action, \"sequence:\", sequence)\n",
    "                cv2.waitKey(500)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frames.append(frame)\n",
    "        out = cv2.VideoWriter(f'videos/signed_videos_temp/{action}/video_{sequence}.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30, (640, 480))\n",
    "        for frame in frames:\n",
    "            out.write(frame)\n",
    "            cv2.imshow('frame', frame)\n",
    "        cv2.waitKey(500)\n",
    "        out.release()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12857277",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73ae6bca",
   "metadata": {},
   "source": [
    "5. Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3096af6c-8ccf-486d-a3db-1c902b6b2eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(no_sequences):\n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                # print(results)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # Apply wait logic \n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,100), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(500)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,100), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe96c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8110134",
   "metadata": {},
   "source": [
    "6. Arrange the folders: If there are already existing dataset on MP_Data similar to MP_Data_2, then run this cell to transfer the videos from MP_Data_2 to MP_Data. This is to avoid overwriting the existing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc2bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "source_folder = \"MP_Data_2\"\n",
    "destination_folder = \"MP_Data\"\n",
    "\n",
    "for folder_name in os.listdir(source_folder):\n",
    "    source_path = os.path.join(source_folder, folder_name)\n",
    "    destination_path = os.path.join(destination_folder, folder_name)\n",
    "    if os.path.isdir(source_path) and os.path.exists(destination_path):\n",
    "        max_number = max([int(file_name.split(\".\")[0]) for file_name in os.listdir(destination_path) if file_name.endswith(\".mp4\")])\n",
    "        for file_name in os.listdir(source_path):\n",
    "            if file_name.endswith(\".mp4\"):\n",
    "                max_number += 1\n",
    "                new_file_name = f\"{max_number}.mp4\"\n",
    "                source_file = os.path.join(source_path, file_name)\n",
    "                destination_file = os.path.join(destination_path, new_file_name)\n",
    "                shutil.move(source_file, destination_file)\n",
    "                \n",
    "# This code will compare the two folders MP_Data and MP_Data_2. If there is a similar name folder in both MP_Data and MP_Data_2, then it will move all .mp4 files from the MP_Data_2 folder to the folder from MP_Data. The name convention will be followed when the .mp4 files are moved. The videos will be numbered after 14.mp4 so the next video is 15.mp4 and the next video transferred is 16.mp4 and so on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
